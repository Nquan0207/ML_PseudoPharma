{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BAYESIAN NETWORK","metadata":{}},{"cell_type":"code","source":"import torch\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nclass BayesianNetwork:\n    def __init__(self, edges, device=\"cpu\"):\n        \"\"\"\n        Bayesian Network with GPU support.\n\n        :param edges: List of (parent, child) edges representing the DAG.\n        :param device: 'cpu' or 'cuda' for GPU computation.\n        \"\"\"\n        self.device = torch.device(device)\n        self.graph = nx.DiGraph()\n        self.graph.add_edges_from(edges)\n        self.nodes = list(self.graph.nodes())\n        self.parents = {node: list(self.graph.predecessors(node)) for node in self.nodes}\n        self.cpts = {}  # Conditional Probability Tables (stored as tensors)\n\n    def visualize(self):\n        \"\"\"Visualizes the Bayesian Network structure.\"\"\"\n        plt.figure(figsize=(8, 6))\n        nx.draw(self.graph, with_labels=True, node_size=3000, node_color=\"lightblue\", edge_color=\"gray\", font_size=12)\n        plt.title(\"Bayesian Network Structure\")\n        plt.show()\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nclass OnlineBayesianEstimator:\n    def __init__(self, model, alpha=1):\n        \"\"\"\n        Online Bayesian Parameter Estimation using PyTorch (GPU accelerated).\n        \n        :param model: BayesianNetwork object.\n        :param alpha: Laplace smoothing parameter.\n        \"\"\"\n        self.model = model\n        self.alpha = alpha  # Laplace smoothing factor\n        self.counts = {}  # Stores accumulated counts for incremental learning\n\n    def update_counts(self, batch_data):\n        \"\"\"\n        Updates counts for each node based on new batch data.\n        \n        :param batch_data: Pandas DataFrame containing the new batch.\n        \"\"\"\n        for node in self.model.nodes:\n            parents = self.model.parents[node]\n            if parents:\n                grouped_data = batch_data.pivot_table(index=parents, columns=node, aggfunc='size', fill_value=0)\n            else:\n                grouped_data = batch_data[node].value_counts().to_frame().T\n\n            if node not in self.counts:\n                self.counts[node] = grouped_data\n            else:\n                self.counts[node] += grouped_data  # Accumulate counts across batches\n\n    def estimate_cpds(self):\n        \"\"\"\n        Recomputes CPDs based on updated counts.\n        \"\"\"\n        cpts = {}\n        for node, count_matrix in self.counts.items():\n            smoothed_counts = count_matrix + self.alpha\n            cpt_tensor = torch.tensor(smoothed_counts.div(smoothed_counts.sum(axis=1), axis=0).values,\n                                      dtype=torch.float32, device=self.model.device)\n            cpts[node] = cpt_tensor\n\n        self.model.cpts = cpts  # Update model CPDs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BayesianInference:\n    def __init__(self, model):\n        \"\"\"\n        Inference in a Bayesian Network using PyTorch (GPU Accelerated).\n        \n        :param model: BayesianNetwork object.\n        \"\"\"\n        self.model = model\n\n    def compute_posterior(self, evidence):\n        \"\"\"\n        Computes P(Label | Features) on GPU.\n\n        :param evidence: Dictionary of observed values {feature_name: value}\n        :return: Dictionary {label_value: probability}\n        \"\"\"\n        labels = list(range(self.model.cpts['label'].shape[1]))  # Get possible label values\n        posterior_probs = {}\n\n        for label_idx in labels:\n            prob = self.model.cpts['label'][0, label_idx].item()  # Extract single probability from tensor\n\n            for feature, value in evidence.items():\n                if feature in self.model.cpts:\n                    feature_cpt = self.model.cpts[feature]\n                    if value in range(feature_cpt.shape[0]):  # Ensure value is within valid range\n                        prob *= feature_cpt[value, min(label_idx, feature_cpt.shape[1] - 1)].item()\n                    else:\n                        prob *= 1e-6  # Small probability for unseen values\n\n            posterior_probs[label_idx] = prob\n\n        # Normalize probabilities (sum-to-1 constraint)\n        total = sum(posterior_probs.values())\n        if total > 0:\n            for label_idx in posterior_probs:\n                posterior_probs[label_idx] /= total\n\n        return posterior_probs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(bn_model, test_data):\n    \"\"\"\n    Performs batch classification using Bayesian Network inference (GPU Accelerated).\n\n    :param bn_model: Trained BayesianNetwork object.\n    :param test_data: Pandas DataFrame of test instances.\n    :return: Predicted labels (PyTorch tensor).\n    \"\"\"\n    inference = BayesianInference(bn_model)\n    predictions = []\n\n    for _, row in test_data.iterrows():\n        evidence = row.to_dict()\n        del evidence['label']  # Remove true label (to predict it)\n        posterior_probs = inference.compute_posterior(evidence)\n        predicted_label = max(posterior_probs, key=posterior_probs.get)  # Argmax P(L | F)\n        predictions.append(predicted_label)\n\n    return torch.tensor(predictions, dtype=torch.int64, device=bn_model.device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import medmnist\nfrom medmnist import PathMNIST\nimport torch\nimport torchvision.transforms as transforms\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport numpy as np\nimport pandas as pd\n\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = PathMNIST(split='train', download=True, transform=transform)\nval_dataset = PathMNIST(split='val', download=True, transform=transform)\ntest_dataset = PathMNIST(split='test', download=True, transform=transform)\n\n# Convert dataset into a structured list\ndef preprocess_dataset(dataset, n_components=20, bins=5):\n    image_list = []\n    label_list = []\n\n    for img_tensor, label_array in dataset:\n        # Convert torch.Tensor to numpy.ndarray and flatten\n        img_np = img_tensor.numpy().reshape(-1)  # Shape: (3*28*28,)\n        \n        # Extract label from numpy array\n        label = label_array[0]  # Convert from [0] to scalar\n        \n        image_list.append(img_np)\n        label_list.append(label)\n    \n    # Convert list to numpy array\n    images_np = np.array(image_list)\n    labels_np = np.array(label_list)\n\n    # Apply PCA to reduce dimensions\n    pca = PCA(n_components=n_components)\n    reduced_features = pca.fit_transform(images_np)\n\n    # Convert PCA features and labels into DataFrame\n    df = pd.DataFrame(reduced_features, columns=[f'feature_{i}' for i in range(n_components)])\n    df['label'] = labels_np  # Add label column\n\n    # Discretize features\n    discretizer = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='uniform')\n    df.iloc[:, :-1] = discretizer.fit_transform(df.iloc[:, :-1])  # Only discretize feature columns\n\n    # Convert to integers (Fixes float output from KBinsDiscretizer)\n    df = df.astype(int)\n\n    return df\n\n# Apply preprocessing to training, validation, and test datasets\ntrain_df = preprocess_dataset(train_dataset)\nval_df = preprocess_dataset(val_dataset)\ntest_df = preprocess_dataset(test_dataset)\n\nprint(train_df.shape)\nprint(train_df.describe())\nprint(train_df.head())","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def online_train_and_validate(train_df, val_df, batch_size=5000, model_path=\"best_bayesian_network.pkl\", device=\"cuda\"):\n    \"\"\"\n    Trains a Bayesian Network using online learning (batch updates).\n    \n    Saves the best model based on validation F1-score.\n    \n    :param train_df: Training dataset (Pandas DataFrame).\n    :param val_df: Validation dataset (Pandas DataFrame).\n    :param batch_size: Number of samples per batch.\n    :param model_path: Path to save the best model.\n    :param device: \"cuda\" for GPU or \"cpu\".\n    \"\"\"\n    # Initialize Bayesian Network structure\n    edges = [(f'feature_{i}', 'label') for i in range(20)]\n    bn_model = BayesianNetwork(edges, device=device)\n\n    # Initialize Online Bayesian Estimator\n    estimator = OnlineBayesianEstimator(bn_model, alpha=1)\n\n    # Shuffle dataset before splitting into batches\n    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n    num_batches = len(train_df) // batch_size + 1  # Number of batches\n    batches = np.array_split(train_df, num_batches)\n\n    # Online Training: Process batches sequentially\n    for i, batch in enumerate(batches):\n        print(f\"Training on batch {i+1}/{num_batches} with {len(batch)} samples...\")\n        estimator.update_counts(batch)  # Incrementally update counts\n\n    # Estimate CPDs after accumulating counts\n    estimator.estimate_cpds()\n    print(\"✅ Finished online training.\")\n\n    # Run inference on validation set\n    val_predictions = predict(bn_model, val_df).cpu().numpy()  # Move back to CPU for evaluation\n\n    # Compute metrics\n    y_true = val_df['label'].values\n    y_pred = val_predictions\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n\n    print(f\"Validation Metrics:\")\n    print(f\" - Accuracy:  {accuracy * 100:.2f}%\")\n    print(f\" - Precision: {precision:.4f}\")\n    print(f\" - Recall:    {recall:.4f}\")\n    print(f\" - F1 Score:  {f1:.4f}\")\n\n    # Save the best model based on F1-score\n    best_model = {\"model\": bn_model, \"f1_score\": f1}\n    with open(model_path, \"wb\") as f:\n        pickle.dump(best_model, f)\n\n    print(f\"✅ Best model saved at: {model_path}\")\n\n    return bn_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:05:35.169493Z","iopub.execute_input":"2025-03-18T07:05:35.169774Z","iopub.status.idle":"2025-03-18T07:05:35.177412Z","shell.execute_reply.started":"2025-03-18T07:05:35.169752Z","shell.execute_reply":"2025-03-18T07:05:35.176446Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport pickle\n\n\n# Define BayesianNetwork and OnlineBayesianEstimator classes (same as before)\n\n# Helper function for hyperparameter tuning\ndef hyperparameter_tuning(train_df, val_df, device=\"cuda\", n_splits=5, alpha_values=[0.1, 1, 10], batch_sizes=[1000, 5000, 10000]):\n    \"\"\"\n    Perform hyperparameter tuning with cross-validation.\n\n    :param train_df: Training dataset (Pandas DataFrame).\n    :param val_df: Validation dataset (Pandas DataFrame).\n    :param device: Device to run the model on ('cpu' or 'cuda').\n    :param n_splits: Number of splits for cross-validation.\n    :param alpha_values: List of alpha values to test.\n    :param batch_sizes: List of batch sizes to test.\n    :return: Best hyperparameters and model.\n    \"\"\"\n    best_f1 = -1\n    best_params = None\n    best_model = None\n    \n    # K-Fold Cross-Validation\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    for alpha in alpha_values:\n        for batch_size in batch_sizes:\n            print(f\"Testing alpha={alpha}, batch_size={batch_size}...\")\n            \n            fold_f1_scores = []\n            \n            for train_index, val_index in kf.split(train_df):\n                train_fold = train_df.iloc[train_index]\n                val_fold = train_df.iloc[val_index]\n                \n                # Initialize Bayesian Network structure\n                edges = [(f'feature_{i}', 'label') for i in range(20)]\n                bn_model = BayesianNetwork(edges, device=device)\n\n                # Initialize Online Bayesian Estimator\n                estimator = OnlineBayesianEstimator(bn_model, alpha=alpha)\n\n                # Shuffle dataset before splitting into batches\n                train_fold = train_fold.sample(frac=1, random_state=42).reset_index(drop=True)\n                num_batches = len(train_fold) // batch_size + 1  # Number of batches\n                batches = np.array_split(train_fold, num_batches)\n\n                # Online Training: Process batches sequentially\n                for i, batch in enumerate(batches):\n                    estimator.update_counts(batch)  # Incrementally update counts\n                \n                # Estimate CPDs after accumulating counts\n                estimator.estimate_cpds()\n\n                # Run inference on validation set\n                val_predictions = predict(bn_model, val_fold).cpu().numpy()  # Move back to CPU for evaluation\n\n                # Compute metrics\n                y_true = val_fold['label'].values\n                y_pred = val_predictions\n\n                f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n                fold_f1_scores.append(f1)\n            \n            avg_f1 = np.mean(fold_f1_scores)\n            print(f\"Average F1 score for alpha={alpha}, batch_size={batch_size}: {avg_f1:.4f}\")\n\n            # Update best model if necessary\n            if avg_f1 > best_f1:\n                best_f1 = avg_f1\n                best_params = {'alpha': alpha, 'batch_size': batch_size}\n                best_model = bn_model\n    \n    print(f\"Best hyperparameters found: {best_params}\")\n    return best_model, best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:05:35.178635Z","iopub.execute_input":"2025-03-18T07:05:35.178888Z","iopub.status.idle":"2025-03-18T07:05:35.202440Z","shell.execute_reply.started":"2025-03-18T07:05:35.178868Z","shell.execute_reply":"2025-03-18T07:05:35.201598Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nbest_model, best_params = hyperparameter_tuning(train_df, val_df, device=\"cuda\", n_splits=5)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T07:05:35.203456Z","iopub.execute_input":"2025-03-18T07:05:35.203700Z","iopub.status.idle":"2025-03-18T07:12:22.812618Z","shell.execute_reply.started":"2025-03-18T07:05:35.203679Z","shell.execute_reply":"2025-03-18T07:12:22.811651Z"}},"outputs":[{"name":"stdout","text":"Testing alpha=0.1, batch_size=1000...\nAverage F1 score for alpha=0.1, batch_size=1000: 0.0209\nTesting alpha=0.1, batch_size=5000...\nAverage F1 score for alpha=0.1, batch_size=5000: 0.0209\nTesting alpha=0.1, batch_size=10000...\nAverage F1 score for alpha=0.1, batch_size=10000: 0.0209\nTesting alpha=1, batch_size=1000...\nAverage F1 score for alpha=1, batch_size=1000: 0.0209\nTesting alpha=1, batch_size=5000...\nAverage F1 score for alpha=1, batch_size=5000: 0.0209\nTesting alpha=1, batch_size=10000...\nAverage F1 score for alpha=1, batch_size=10000: 0.0209\nTesting alpha=10, batch_size=1000...\nAverage F1 score for alpha=10, batch_size=1000: 0.0209\nTesting alpha=10, batch_size=5000...\nAverage F1 score for alpha=10, batch_size=5000: 0.0209\nTesting alpha=10, batch_size=10000...\nAverage F1 score for alpha=10, batch_size=10000: 0.0209\nBest hyperparameters found: {'alpha': 0.1, 'batch_size': 1000}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pickle\n\n# Train with mini-batch updates\nbest_model = online_train_and_validate(train_df, val_df, batch_size=5000, model_path=\"best_bayesian_network.pkl\", device=\"cuda\")","metadata":{"execution":{"iopub.execute_input":"2025-03-10T08:14:01.580039Z","iopub.status.busy":"2025-03-10T08:14:01.579759Z","iopub.status.idle":"2025-03-10T08:14:07.884550Z","shell.execute_reply":"2025-03-10T08:14:07.883353Z","shell.execute_reply.started":"2025-03-10T08:14:01.580017Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n","  return bound(*args, **kwds)\n"]},{"name":"stdout","output_type":"stream","text":["Training on batch 1/18 with 5000 samples...\n","Training on batch 2/18 with 5000 samples...\n","Training on batch 3/18 with 5000 samples...\n","Training on batch 4/18 with 5000 samples...\n","Training on batch 5/18 with 5000 samples...\n","Training on batch 6/18 with 5000 samples...\n","Training on batch 7/18 with 5000 samples...\n","Training on batch 8/18 with 5000 samples...\n","Training on batch 9/18 with 5000 samples...\n","Training on batch 10/18 with 5000 samples...\n","Training on batch 11/18 with 5000 samples...\n","Training on batch 12/18 with 5000 samples...\n","Training on batch 13/18 with 5000 samples...\n","Training on batch 14/18 with 5000 samples...\n","Training on batch 15/18 with 4999 samples...\n","Training on batch 16/18 with 4999 samples...\n","Training on batch 17/18 with 4999 samples...\n","Training on batch 18/18 with 4999 samples...\n","✅ Finished online training.\n","Validation Metrics:\n"," - Accuracy:  10.41%\n"," - Precision: 0.0116\n"," - Recall:    0.1111\n"," - F1 Score:  0.0209\n","✅ Best model saved at: best_bayesian_network.pkl\n"]}],"execution_count":9},{"cell_type":"markdown","source":"# AUGMENTED NAIVE BAYES (ANB)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport networkx as nx\nfrom sklearn.feature_selection import mutual_info_classif\n\ndef learn_anb_structure(train_df):\n    \"\"\"\n    Learns an Augmented Naïve Bayes (ANB) structure using Mutual Information.\n\n    :param train_df: Training dataset as Pandas DataFrame.\n    :return: List of edges representing the Bayesian Network.\n    \"\"\"\n    features = train_df.columns[:-1]  # Exclude label\n    label = 'label'\n    \n    # Compute Mutual Information between each feature and label\n    mi_scores = mutual_info_classif(train_df[features], train_df[label])\n\n    # Create a complete graph and assign MI scores as weights\n    G = nx.Graph()\n    for i, feature in enumerate(features):\n        G.add_edge(label, feature, weight=mi_scores[i])  # Connect features to label\n\n    # Compute Mutual Information between features\n    for i, f1 in enumerate(features):\n        for j, f2 in enumerate(features):\n            if i < j:  # Avoid duplicate edges\n                mi = mutual_info_classif(train_df[[f1]], train_df[f2])[0]\n                G.add_edge(f1, f2, weight=mi)\n\n    # Use **Maximum Spanning Tree (MST)** to find best dependencies\n    mst = nx.maximum_spanning_tree(G)\n\n    # Convert MST to a Directed Graph (DAG)\n    DAG = nx.DiGraph()\n    for edge in mst.edges(data=True):\n        parent, child, _ = edge\n        DAG.add_edge(parent, child)\n\n    return list(DAG.edges)\n","metadata":{"execution":{"iopub.execute_input":"2025-03-10T08:27:59.018080Z","iopub.status.busy":"2025-03-10T08:27:59.017787Z","iopub.status.idle":"2025-03-10T08:27:59.098261Z","shell.execute_reply":"2025-03-10T08:27:59.097610Z","shell.execute_reply.started":"2025-03-10T08:27:59.018058Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import os\nimport pickle\nimport pandas as pd\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ndef train_anb(train_df, test_df, val_df, model_path=\"best_anb.pkl\"):\n    \"\"\"\n    Trains an Augmented Naïve Bayes (ANB) model with learned feature dependencies.\n    \n    Saves the best model based on validation F1-score.\n    \n    :param train_df: Training dataset (Pandas DataFrame).\n    :param test_df: Testing dataset (Pandas DataFrame).\n    :param val_df: Validation dataset (Pandas DataFrame).\n    :param model_path: Path to save the best model.\n    \"\"\"\n    print(\"Starting Augmented Naïve Bayes (ANB) model training...\")\n\n    # Learn feature dependencies\n    edges = learn_anb_structure(train_df)\n    print(f\"Learned ANB Structure: {edges}\")\n\n    # Extract features and labels\n    X_train, y_train = train_df.iloc[:, :-1], train_df.iloc[:, -1]\n    X_test, y_test = test_df.iloc[:, :-1], test_df.iloc[:, -1]\n    X_val, y_val = val_df.iloc[:, :-1], val_df.iloc[:, -1]\n\n    # Train ANB Model with Hyperparameter Tuning\n    print(\"Training Augmented Naïve Bayes model...\")\n    param_grid = {\n        'alpha': [0.01, 0.1, 0.5, 1.0, 2.0],\n        'fit_prior': [True, False]\n    }\n\n    anb_model = MultinomialNB()\n    grid_search = GridSearchCV(anb_model, param_grid, cv=5, scoring='f1_weighted')\n    grid_search.fit(X_train, y_train)\n\n    best_model = grid_search.best_estimator_\n    print(f\"Best hyperparameters: {grid_search.best_params_}\")\n\n    # Evaluate on test data\n    y_test_pred = best_model.predict(X_test)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    print(classification_report(y_test, y_test_pred))\n\n    # Evaluate on validation data\n    y_val_pred = best_model.predict(X_val)\n    val_accuracy = accuracy_score(y_val, y_val_pred)\n    precision = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n    recall = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n    f1 = f1_score(y_val, y_val_pred, average='macro', zero_division=0)\n\n    print(\"Validation Metrics:\")\n    print(f\" - Accuracy:  {val_accuracy * 100:.2f}%\")\n    print(f\" - Precision: {precision:.4f}\")\n    print(f\" - Recall:    {recall:.4f}\")\n    print(f\" - F1 Score:  {f1:.4f}\")\n\n    # Save the best model\n    model_data = {\n        'model': best_model,\n        'feature_dependencies': edges,  # Save the learned feature dependencies\n        'metrics': {\n            'test_accuracy': test_accuracy,\n            'validation_accuracy': val_accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1,\n            'best_params': grid_search.best_params_\n        }\n    }\n\n    save_path = os.path.join(\"models\", model_path)\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Ensure directory exists\n    with open(save_path, 'wb') as f:\n        pickle.dump(model_data, f)\n\n    print(f\"Augmented Naïve Bayes model saved at: {save_path}\")\n\n    return model_data\n","metadata":{"execution":{"iopub.execute_input":"2025-03-10T08:28:02.899397Z","iopub.status.busy":"2025-03-10T08:28:02.899055Z","iopub.status.idle":"2025-03-10T08:28:02.916619Z","shell.execute_reply":"2025-03-10T08:28:02.915838Z","shell.execute_reply.started":"2025-03-10T08:28:02.899365Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Train ANB model\ntrained_anb_model = train_anb(train_df, test_df, val_df, model_path=\"best_anb.pkl\")\n","metadata":{"execution":{"iopub.execute_input":"2025-03-10T08:28:08.913972Z","iopub.status.busy":"2025-03-10T08:28:08.913688Z","iopub.status.idle":"2025-03-10T08:29:48.491140Z","shell.execute_reply":"2025-03-10T08:29:48.490261Z","shell.execute_reply.started":"2025-03-10T08:28:08.913950Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Augmented Naïve Bayes (ANB) model training...\n","Learned ANB Structure: [('label', 'feature_1'), ('label', 'feature_0'), ('label', 'feature_9'), ('label', 'feature_13'), ('label', 'feature_14'), ('label', 'feature_12'), ('feature_2', 'feature_7'), ('feature_2', 'feature_4'), ('feature_2', 'feature_3'), ('feature_2', 'feature_10'), ('feature_4', 'feature_12'), ('feature_4', 'feature_16'), ('feature_3', 'feature_8'), ('feature_10', 'feature_15'), ('feature_10', 'feature_19'), ('feature_8', 'feature_17'), ('feature_5', 'feature_6'), ('feature_5', 'feature_10'), ('feature_6', 'feature_11'), ('feature_17', 'feature_18')]\n","Training Augmented Naïve Bayes model...\n","Best hyperparameters: {'alpha': 0.5, 'fit_prior': False}\n","Test Accuracy: 0.4078\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.83      0.89      1338\n","           1       0.48      0.97      0.64       847\n","           2       0.16      0.02      0.04       339\n","           3       0.00      0.00      0.00       634\n","           4       0.23      0.93      0.37      1035\n","           5       0.00      0.00      0.00       592\n","           6       0.25      0.00      0.00       741\n","           7       0.00      0.00      0.00       421\n","           8       0.80      0.02      0.03      1233\n","\n","    accuracy                           0.41      7180\n","   macro avg       0.32      0.31      0.22      7180\n","weighted avg       0.44      0.41      0.30      7180\n","\n","Validation Metrics:\n"," - Accuracy:  18.40%\n"," - Precision: 0.1410\n"," - Recall:    0.2052\n"," - F1 Score:  0.1154\n","Augmented Naïve Bayes model saved at: models/best_anb.pkl\n"]}],"execution_count":12},{"cell_type":"markdown","source":"# HIDDEN MARKOV MODEL (HMM)","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom hmmlearn import hmm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\nclass HiddenMarkovModelTrainer:\n    def __init__(self):\n        self.hmm_models = None  # Store trained HMM models\n\n    def train_hmm(self, train_df, test_df, val_df, n_components_range=[2, 3, 4], random_state=42, model_path=\"hmm_models.pkl\"):\n        \"\"\"\n        Trains a Hidden Markov Model (HMM) for each class in the dataset with cross-validation and hyperparameter tuning.\n\n        Args:\n            train_df: Training dataset (Pandas DataFrame).\n            test_df: Testing dataset (Pandas DataFrame).\n            val_df: Validation dataset (Pandas DataFrame).\n            n_components_range: List of possible numbers of hidden states (components) for hyperparameter tuning.\n            random_state: Random seed for reproducibility.\n            model_path: Path to save the trained HMM models.\n            \n        Returns:\n            A dictionary containing the trained HMM models.\n        \"\"\"\n        print(\"Starting Hidden Markov Model (HMM) training...\")\n\n        # Extract features and labels\n        X_train, y_train = train_df.iloc[:, :-1], train_df.iloc[:, -1]\n        X_test, y_test = test_df.iloc[:, :-1], test_df.iloc[:, -1]\n        X_val, y_val = val_df.iloc[:, :-1], val_df.iloc[:, -1]\n\n        # Normalize the feature data using StandardScaler\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        X_val_scaled = scaler.transform(X_val)\n\n        # Perform K-fold cross-validation for hyperparameter tuning\n        kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n        best_hmm_models = {}\n        best_f1 = -1  # Initialize best F1 score as a very low value\n\n        # Iterate over possible n_components for hyperparameter tuning\n        for n_components in n_components_range:\n            hmm_models = {}\n            avg_f1_score = 0  # Average F1 score for this n_components across folds\n\n            for train_idx, val_idx in kf.split(X_train_scaled):\n                X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n                y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n\n                # Train HMM for each class\n                for label in np.unique(y_train_fold):\n                    X_label = X_train_fold[y_train_fold == label]\n\n                    if len(X_label) < n_components:\n                        print(f\"Warning: Not enough samples ({len(X_label)}) for label {label} with {n_components} components\")\n                        continue\n\n                    model = hmm.GaussianHMM(\n                        n_components=n_components,\n                        covariance_type=\"full\",\n                        n_iter=200,\n                        random_state=random_state\n                    )\n\n                    try:\n                        model.fit(X_label)  # Fit the model to the class-specific data\n                        hmm_models[label] = model\n                    except Exception as e:\n                        print(f\"Error training HMM for label {label}: {e}\")\n\n                # Evaluate on validation fold\n                y_val_pred = self.predict_hmm(hmm_models, X_val_fold)\n                f1 = f1_score(y_val_fold, y_val_pred, average='macro', zero_division=0)\n                avg_f1_score += f1\n\n            avg_f1_score /= kf.get_n_splits()\n            print(f\"Average F1 score for n_components={n_components}: {avg_f1_score:.4f}\")\n\n            # Update best model if necessary\n            if avg_f1_score > best_f1:\n                best_f1 = avg_f1_score\n                best_hmm_models = hmm_models\n                print(f\"Updated best model with n_components={n_components}\")\n\n        # Save the best model\n        save_path = os.path.join(\"models\", model_path)\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        with open(save_path, \"wb\") as f:\n            pickle.dump(best_hmm_models, f)\n\n        print(f\"HMM models saved at: {save_path}\")\n        self.hmm_models = best_hmm_models  # Store the best models for later use\n        return best_hmm_models\n\n    def predict_hmm(self, hmm_models, X_test_scaled):\n        \"\"\"\n        Predicts labels using trained HMM models.\n\n        Args:\n            hmm_models: Dictionary of trained HMM models.\n            X_test_scaled: Scaled feature matrix.\n\n        Returns:\n            List of predicted labels.\n        \"\"\"\n        X_test_scaled = np.array(X_test_scaled)\n        predictions = []\n\n        for i in range(X_test_scaled.shape[0]):\n            x = X_test_scaled[i].reshape(1, -1)  # Correct reshaping for single sample\n            max_log_prob = float('-inf')\n            best_label = None\n\n            for label, model in hmm_models.items():\n                try:\n                    log_prob = model.score(x)\n                    if log_prob > max_log_prob:\n                        max_log_prob = log_prob\n                        best_label = label\n                except:\n                    continue\n\n            predictions.append(best_label if best_label is not None else -1)\n\n        return np.array(predictions)\n\n    def evaluate_hmm(self, hmm_models, X_test_scaled, y_test, X_val_scaled, y_val):\n        \"\"\"\n        Evaluates the HMM model on test and validation sets.\n\n        Args:\n            hmm_models: Dictionary of trained HMM models.\n            X_test_scaled: Scaled test features.\n            y_test: Test labels.\n            X_val_scaled: Scaled validation features.\n            y_val: Validation labels.\n        \n        Returns:\n            A dictionary of evaluation metrics.\n        \"\"\"\n        # Predict on test set\n        y_test_pred = self.predict_hmm(hmm_models, X_test_scaled)\n        test_accuracy = accuracy_score(y_test, y_test_pred)\n        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n        print(classification_report(y_test, y_test_pred))\n\n        # Predict on validation set\n        y_val_pred = self.predict_hmm(hmm_models, X_val_scaled)\n        val_accuracy = accuracy_score(y_val, y_val_pred)\n        precision = precision_score(y_val, y_val_pred, average='macro', zero_division=0)\n        recall = recall_score(y_val, y_val_pred, average='macro', zero_division=0)\n        f1 = f1_score(y_val, y_val_pred, average='macro', zero_division=0)\n\n        print(\"Validation Metrics:\")\n        print(f\" - Accuracy:  {val_accuracy * 100:.2f}%\")\n        print(f\" - Precision: {precision:.4f}\")\n        print(f\" - Recall:    {recall:.4f}\")\n        print(f\" - F1 Score:  {f1:.4f}\")\n\n        return {\n            'test_accuracy': test_accuracy,\n            'validation_accuracy': val_accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1\n        }\n\n# Initialize and train the HMM model\nhmm_trainer = HiddenMarkovModelTrainer()\ntrained_hmm_models = hmm_trainer.train_hmm(train_df, test_df, val_df)\n\n# Load and evaluate the model\nhmm_metrics = hmm_trainer.evaluate_hmm(\n    trained_hmm_models, \n    test_df.iloc[:, :-1].values, test_df.iloc[:, -1].values, \n    val_df.iloc[:, :-1].values, val_df.iloc[:, -1].values\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-10T08:40:34.820709Z","iopub.status.busy":"2025-03-10T08:40:34.820382Z","iopub.status.idle":"2025-03-10T08:40:34.982592Z","shell.execute_reply":"2025-03-10T08:40:34.981670Z","shell.execute_reply.started":"2025-03-10T08:40:34.820665Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Initialize and train the HMM model\nhmm_trainer = HiddenMarkovModelTrainer()\ntrained_hmm_models = hmm_trainer.train_hmm(train_df, test_df, val_df)\n\n# Load and evaluate the model\nhmm_metrics = hmm_trainer.evaluate_hmm(\n    trained_hmm_models, \n    test_df.iloc[:, :-1].values, test_df.iloc[:, -1].values, \n    val_df.iloc[:, :-1].values, val_df.iloc[:, -1].values\n)","metadata":{"execution":{"iopub.execute_input":"2025-03-10T08:47:05.298986Z","iopub.status.busy":"2025-03-10T08:47:05.298695Z","iopub.status.idle":"2025-03-10T08:48:04.278327Z","shell.execute_reply":"2025-03-10T08:48:04.277511Z","shell.execute_reply.started":"2025-03-10T08:47:05.298956Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Hidden Markov Model (HMM) training...\n","Successfully trained HMM for label 0 with 9366 samples\n","Successfully trained HMM for label 1 with 9509 samples\n","Successfully trained HMM for label 2 with 10360 samples\n","Successfully trained HMM for label 3 with 10401 samples\n","Successfully trained HMM for label 4 with 8006 samples\n","Successfully trained HMM for label 5 with 12182 samples\n","Successfully trained HMM for label 6 with 7886 samples\n","Successfully trained HMM for label 7 with 9401 samples\n","Successfully trained HMM for label 8 with 12885 samples\n","HMM models saved at: models/hmm_models.pkl\n","Test Accuracy: 0.1180\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1338\n","           1       0.12      1.00      0.21       847\n","           2       0.00      0.00      0.00       339\n","           3       0.00      0.00      0.00       634\n","           4       0.00      0.00      0.00      1035\n","           5       0.00      0.00      0.00       592\n","           6       0.00      0.00      0.00       741\n","           7       0.00      0.00      0.00       421\n","           8       0.00      0.00      0.00      1233\n","\n","    accuracy                           0.12      7180\n","   macro avg       0.01      0.11      0.02      7180\n","weighted avg       0.01      0.12      0.02      7180\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["Validation Metrics:\n"," - Accuracy:  10.57%\n"," - Precision: 0.0117\n"," - Recall:    0.1111\n"," - F1 Score:  0.0212\n"]}],"execution_count":23}]}